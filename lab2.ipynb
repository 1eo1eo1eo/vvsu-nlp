{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cf83889029c3a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:21.935446Z",
     "start_time": "2025-03-15T01:54:21.932437Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger  # noqa\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:21.949201Z",
     "start_time": "2025-03-15T01:54:21.945294Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# открываем файл\n",
    "\n",
    "filepath: str = \"your filepath\"\n",
    "\n",
    "with open(filepath, \"r\") as file:\n",
    "    text: str = file.read().replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "167eb9dbe35142df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:22.268592Z",
     "start_time": "2025-03-15T01:54:21.965120Z"
    }
   },
   "outputs": [],
   "source": [
    "# подготавливаем текст\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def preprocess_text(paragraphs: list[str]) -> list[list[str]]:\n",
    "    preprocessed = []\n",
    "    for paragraph in paragraphs:\n",
    "        doc = Doc(paragraph)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos in (\"PUNCT\", \"NUM\"):\n",
    "                continue\n",
    "\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemma = token.lemma.lower()\n",
    "\n",
    "            if lemma not in stopwords:\n",
    "                tokens.append(lemma)\n",
    "        preprocessed.append(tokens)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57f3e82d1b5d4246",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:24.784555Z",
     "start_time": "2025-03-15T01:54:22.284886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_text:  ['преступление', 'наказание', 'федор', 'михаилович', 'достоевский', 'смерть', 'спасение', 'родион', 'раскольников', 'это', 'роман', 'петербургский', 'студент', 'родион', 'раскольников']\n"
     ]
    }
   ],
   "source": [
    "# делим текст на параграфы\n",
    "\n",
    "\n",
    "def split_text_to_sentences(text: str) -> list[str]:\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def group_sentences(\n",
    "    sentences: list[str],\n",
    "    group_size: int = 200,\n",
    ") -> list[str]:\n",
    "    groups = []\n",
    "    for i in range(0, len(sentences), group_size):\n",
    "        group = \" \".join(sentences[i : i + group_size])\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "\n",
    "sentences = split_text_to_sentences(text)\n",
    "\n",
    "group = group_sentences(sentences)\n",
    "\n",
    "preprocessed_paragraphs = preprocess_text(group)\n",
    "\n",
    "print(\"preprocessed_text: \", preprocessed_paragraphs[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b82e123683a89921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:24.798884Z",
     "start_time": "2025-03-15T01:54:24.797269Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(docs: list) -> list[str]:\n",
    "    vocab_set: set[str] = set()\n",
    "    for tokens in docs:\n",
    "        vocab_set.update(tokens)\n",
    "    vocabulary: list[str] = sorted(list(vocab_set))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81696f9601084ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:25.408890Z",
     "start_time": "2025-03-15T01:54:24.811146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "matrix:  [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# bag_of_words\n",
    "\n",
    "\n",
    "def bag_of_words(text: list[str]) -> list[int]:\n",
    "    vocabulary = get_vocabulary(text)\n",
    "\n",
    "    bow_matrix = []\n",
    "    for tokens in text:\n",
    "        row = [0] * len(vocabulary)\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                j = vocabulary.index(token)\n",
    "                row[j] += 1\n",
    "        bow_matrix.append(row)\n",
    "\n",
    "    return bow_matrix\n",
    "\n",
    "\n",
    "matrix = bag_of_words(preprocessed_paragraphs[:10])\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(\"matrix: \", matrix[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7044edb08a0b80e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:54:26.278338Z",
     "start_time": "2025-03-15T01:54:25.421217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ---\n",
      "Словарь: ['1789', 'confessions', 'contra', 'danke', 'i', 'ich', 'ii', 'iii', 'iv', 'pro', 'v', 'vi', 'vii', 'xix', 'а-а-а', 'абсолютно', 'авдотья', 'автор', 'адам', 'адвокатский', 'адрес', 'адрес-то', 'адский', 'ажить', 'аккуратно', 'аксиома', 'акцент', 'александр', 'алена', 'алена-то', 'али', 'аллея', 'аль', 'аля', 'амалия', 'амбиция', 'амна', 'ан', 'ана', 'анализ', 'анаполнять', 'ангел', 'англия', 'анна', 'аон', 'апожалеть', 'апосмотреть', 'аппетит', 'арестовать', 'арестовывать', 'арифметика', 'аркадий', 'армяк', 'артельный', 'аршин', 'асердце', 'асессорша', 'асестра', 'ася', 'ата', 'атака', 'атласный', 'атогда', 'атут', 'афанасиевич', 'афанасий', 'афанасий-то', 'африка', 'ах', 'ахать', 'ая', 'б', 'ба', 'баба', 'бабенка', 'бабий', 'бабушка', 'бабушкин', 'багрово-красная', 'багрово-красный', 'бакен', 'балалайка', 'балкон', 'банька', 'барк', 'барышня', 'бастилия', 'батюшка', 'бахрома', 'бахрушин', 'бахус', 'бацилла', 'башмак', 'башмаки-с', 'бег', 'бегать', 'беда', 'бедно', 'бедность', 'бедный']\n",
      "Матрица TF-IDF:\n",
      "[0.0007211535506593646, 0.0, 0.0005760336121811382, 0.0, 0.00047306937723060825, 0.0, 0.0, 0.0, 0.0, 0.0005760336121811382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0005760336121811382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0005455547974566189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0, 0.001419208131691825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004499704076037037, 0.0, 0.00047306937723060825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001572816447627931, 0.0005455547974566189]\n"
     ]
    }
   ],
   "source": [
    "# tf_idf\n",
    "\n",
    "\n",
    "def compute_df(\n",
    "    vocabulary: list[str],\n",
    "    docs: list[list],\n",
    ") -> list[int]:\n",
    "    df: list[int] = [0] * len(vocabulary)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        for tokens in docs:\n",
    "            if word in tokens:\n",
    "                df[i] += 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_idf(\n",
    "    df: list[int],\n",
    "    N: int,\n",
    ") -> list[float]:\n",
    "    idf: list[float] = [math.log(N / (1 + df_val)) for df_val in df]\n",
    "    return idf\n",
    "\n",
    "\n",
    "def compute_tf(\n",
    "    doc: list[str],\n",
    "    vocabulary: list[str],\n",
    ") -> list[float]:\n",
    "    total_words: int = len(doc)\n",
    "    word_counts: dict[str, int] = {}\n",
    "    for token in doc:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1\n",
    "\n",
    "    tf: list[float] = []\n",
    "    for word in vocabulary:\n",
    "        count: int = word_counts.get(word, 0)\n",
    "        tf_val: float = count / total_words if total_words > 0 else 0\n",
    "        tf.append(tf_val)\n",
    "    return tf\n",
    "\n",
    "\n",
    "def compute_tf_idf_for_doc(\n",
    "    doc: list[str],\n",
    "    vocabulary: list[str],\n",
    "    idf: list[float],\n",
    ") -> list[float]:\n",
    "    tf: list[float] = compute_tf(doc, vocabulary)\n",
    "    tf_idf: list[float] = [tf_val * idf_val for tf_val, idf_val in zip(tf, idf)]\n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def tf_idf(docs: list[str]) -> tuple:\n",
    "    vocabulary = get_vocabulary(docs)\n",
    "    N = len(docs)\n",
    "    df = compute_df(vocabulary, docs)\n",
    "    idf = compute_idf(df, N)\n",
    "\n",
    "    tfidf_matrix: list[list] = []\n",
    "    for doc in docs:\n",
    "        tfidf_vector: list[float] = compute_tf_idf_for_doc(doc, vocabulary, idf)\n",
    "        tfidf_matrix.append(tfidf_vector)\n",
    "\n",
    "    return vocabulary, tfidf_matrix\n",
    "\n",
    "\n",
    "vocab_tfidf, tfidf_matrix = tf_idf(preprocessed_paragraphs)\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(\"Словарь:\", vocab_tfidf[:100])\n",
    "print(\"Матрица TF-IDF:\")\n",
    "print(tfidf_matrix[0][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
