{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T01:38:13.886998Z",
     "start_time": "2025-03-15T01:38:11.090398Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "filepath: str = \"your filepaht\"\n",
    "\n",
    "with open(filepath, \"r\") as file:\n",
    "    text: str = file.read().replace(\"\\n\", \" \")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def preprocess_text(paragraphs: list[str]) -> list[list[str]]:\n",
    "    preprocessed = []\n",
    "    for paragraph in paragraphs:\n",
    "        doc = Doc(paragraph)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "\n",
    "        tokens = []\n",
    "        for token in doc.tokens:\n",
    "            if token.pos in (\"PUNCT\", \"NUM\"):\n",
    "                continue\n",
    "\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemma = token.lemma.lower()\n",
    "\n",
    "            if lemma not in stopwords:\n",
    "                tokens.append(lemma)\n",
    "        preprocessed.append(tokens)\n",
    "    return preprocessed\n",
    "\n",
    "def split_text_to_sentences(text: str) -> list[str]:\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    return sentences\n",
    "\n",
    "def group_sentences(sentences: list[str], group_size: int = 200) -> list[str]:\n",
    "    groups = []\n",
    "    for i in range(0, len(sentences), group_size):\n",
    "        group = \" \".join(sentences[i:i+group_size])\n",
    "        groups.append(group)\n",
    "    return groups\n",
    "\n",
    "sentences = split_text_to_sentences(text)\n",
    "\n",
    "group = group_sentences(sentences)\n",
    "\n",
    "preprocessed_paragraphs = preprocess_text(group)\n",
    "\n",
    "print(\"preprocessed_text: \", preprocessed_paragraphs[0][:15])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed_text:  ['преступление', 'наказание', 'федор', 'михаилович', 'достоевский', 'смерть', 'спасение', 'родион', 'раскольников', 'это', 'роман', 'петербургский', 'студент', 'родион', 'раскольников']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:38:14.460593Z",
     "start_time": "2025-03-15T01:38:13.890294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bag_of_words(text: list[str]) -> list[int]:\n",
    "    vocab_set = set()\n",
    "    for tokens in text:\n",
    "        vocab_set.update(tokens)\n",
    "    vocabulary = sorted(list(vocab_set))\n",
    "\n",
    "    bow_matrix = []\n",
    "    for tokens in text:\n",
    "        row = [0] * len(vocabulary)\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                j = vocabulary.index(token)\n",
    "                row[j] += 1\n",
    "        bow_matrix.append(row)\n",
    "\n",
    "    return bow_matrix\n",
    "\n",
    "\n",
    "matrix = bag_of_words(preprocessed_paragraphs[:10])\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(\"matrix: \", matrix[0][:100])"
   ],
   "id": "81696f9601084ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "matrix:  [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T01:38:15.275948Z",
     "start_time": "2025-03-15T01:38:14.474282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tf_idf(text: list[str]) -> tuple:\n",
    "    vocab_set = set()\n",
    "    for tokens in text:\n",
    "        vocab_set.update(tokens)\n",
    "    vocabulary = sorted(list(vocab_set))\n",
    "    N = len(text)\n",
    "\n",
    "    df = [0] * len(vocabulary)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        for tokens in text:\n",
    "            if word in tokens:\n",
    "                df[i] += 1\n",
    "\n",
    "    idf = [math.log(N / (1 + df[i])) for i in range(len(vocabulary))]\n",
    "\n",
    "    tfidf_matrix = []\n",
    "    for tokens in text:\n",
    "        total_words = len(tokens)\n",
    "        word_counts = {}\n",
    "        for t in tokens:\n",
    "            word_counts[t] = word_counts.get(t, 0) + 1\n",
    "\n",
    "        row = []\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            tf = word_counts.get(word, 0) / total_words if total_words > 0 else 0\n",
    "            tfidf_value = tf * idf[i]\n",
    "            row.append(tfidf_value)\n",
    "        tfidf_matrix.append(row)\n",
    "\n",
    "    return vocabulary, tfidf_matrix\n",
    "\n",
    "vocab_tfidf, tfidf_matrix = tf_idf(preprocessed_paragraphs)\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(\"Словарь:\", vocab_tfidf[:100])\n",
    "print(\"Матрица TF-IDF:\")\n",
    "print(tfidf_matrix[0][:100])\n"
   ],
   "id": "7044edb08a0b80e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ---\n",
      "Словарь: ['1789', 'confessions', 'contra', 'danke', 'i', 'ich', 'ii', 'iii', 'iv', 'pro', 'v', 'vi', 'vii', 'xix', 'а-а-а', 'абсолютно', 'авдотья', 'автор', 'адам', 'адвокатский', 'адрес', 'адрес-то', 'адский', 'ажить', 'аккуратно', 'аксиома', 'акцент', 'александр', 'алена', 'алена-то', 'али', 'аллея', 'аль', 'аля', 'амалия', 'амбиция', 'амна', 'ан', 'ана', 'анализ', 'анаполнять', 'ангел', 'англия', 'анна', 'аон', 'апожалеть', 'апосмотреть', 'аппетит', 'арестовать', 'арестовывать', 'арифметика', 'аркадий', 'армяк', 'артельный', 'аршин', 'асердце', 'асессорша', 'асестра', 'ася', 'ата', 'атака', 'атласный', 'атогда', 'атут', 'афанасиевич', 'афанасий', 'афанасий-то', 'африка', 'ах', 'ахать', 'ая', 'б', 'ба', 'баба', 'бабенка', 'бабий', 'бабушка', 'бабушкин', 'багрово-красная', 'багрово-красный', 'бакен', 'балалайка', 'балкон', 'банька', 'барк', 'барышня', 'бастилия', 'батюшка', 'бахрома', 'бахрушин', 'бахус', 'бацилла', 'башмак', 'башмаки-с', 'бег', 'бегать', 'беда', 'бедно', 'бедность', 'бедный']\n",
      "Матрица TF-IDF:\n",
      "[0.0007211535506593646, 0.0, 0.0005760336121811382, 0.0, 0.00047306937723060825, 0.0, 0.0, 0.0, 0.0, 0.0005760336121811382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0005760336121811382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0005455547974566189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0, 0.001419208131691825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004499704076037037, 0.0, 0.00047306937723060825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0007211535506593646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001572816447627931, 0.0005455547974566189]\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
